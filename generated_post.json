{
  "title": "窥探AI大脑：system_prompts_leaks 项目如何揭开主流聊天机器人的“系统提示”面纱 🤖🔍",
  "content": "窥探AI大脑：system_prompts_leaks 项目如何揭开主流聊天机器人的“系统提示”面纱 🤖🔍\n<h2 id=\"the-curtain-behind-the-ai\">AI幕后的“导演脚本”</h2>\n<p>你是否曾好奇，当你向ChatGPT提问时，它为何总是以那种礼貌、乐于助人且略带谨慎的口吻回应？当Claude拒绝回答某些敏感问题时，它内心的“准则”究竟是什么？我们与这些AI的每一次对话，看似自由，实则都在一个无形的框架内进行。这个框架，就是它们的“系统提示”（System Prompt）。</p>\n<p>想象一下，你正在面试一位全能助理。在面试开始前，老板偷偷塞给助理一张纸条，上面写着：“你是一位乐于助人且无害的助手。不要生成暴力、仇恨或非法内容。如果用户请求不当，请礼貌拒绝并解释原因。”这张纸条，就是助理的“系统提示”。它从根本上塑造了助理的行为模式，但作为用户的我们，通常对此一无所知。</p>\n<p>这正是开发者 <strong>asgeirtj</strong> 创建 <a href=\"https://github.com/asgeirtj/system_prompts_leaks\"><code>system_prompts_leaks</code></a> 项目的初衷。这个项目是一个精心收集的仓库，里面存放着从ChatGPT、Claude、Gemini等流行聊天机器人中“提取”出来的系统提示。它就像一本AI的“用户手册”，让我们得以一窥这些强大模型背后的“出厂设置”。</p>\n\n<h2 id=\"why-it-matters\">为何“系统提示”如此重要？</h2>\n<p>对于普通用户，系统提示可能只是一个确保AI安全、有用的背景设定。但对于开发者、研究人员和AI爱好者而言，理解系统提示至关重要：</p>\n<ul>\n<li><strong>🎯 提示工程（Prompt Engineering）的基石</strong>：要有效地与AI对话或构建基于AI的应用，你必须知道它的“初始状态”。了解系统提示能帮助你设计出更高效、更能绕过限制的用户提示。</li>\n<li><strong>🔬 模型行为分析的窗口</strong>：不同AI公司的安全策略、价值观和产品定位，都浓缩在这段初始指令中。通过对比分析，我们可以理解为何Claude在某些问题上比ChatGPT更保守，或者Gemini的回答风格有何不同。</li>\n<li><strong>🛠️ 本地模型调优的参考</strong>：对于使用开源大模型（如Llama、Mistral）的开发者，这些顶尖商业模型的系统提示是极佳的参考模板，可以用来配置自己的聊天助手，使其行为更接近成熟产品。</li>\n<li><strong>💡 教育与研究价值</strong>：它是学习如何与大语言模型设定边界、引导对话的绝佳案例。</li>\n</ul>\n<p>然而，这些信息通常被AI服务提供商视为商业机密或安全核心，不会主动公开。<code>system_prompts_leaks</code> 项目通过技术手段（如提示注入、越狱或从早期版本、API文档中挖掘）收集这些信息，并将其开源，填补了这一知识空白。</p>\n\n<h2 id=\"a-peek-into-the-collection\">窥探宝库：项目内容一览</h2>\n<p>打开项目的GitHub仓库，你会发现一个按模型分类的、结构清晰的文档集合。让我们看看其中一些有趣的片段：</p>\n<h3 id=\"chatgpt-style\">ChatGPT的“人格”设定</h3>\n<p>从仓库中一份据信是ChatGPT使用的系统提示中，我们可以看到其核心指令：</p>\n<pre><code class=\"language-markdown\">You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: {knowledge_cutoff}\nCurrent date: {current_date}\n\n# 核心准则\n- 你是乐于助人、无害且诚实的助手。\n- 如果用户请求不当，礼貌地拒绝，并解释你的安全政策。\n- 避免生成暴力、仇恨、自残、性内容或高度政治化的内容。\n- 承认你的局限性。如果你不知道，就说不知道。不要编造信息。\n- 使用清晰、结构化的Markdown格式进行回复以增强可读性。\n- 除非用户明确要求，否则默认使用英语思考和回复。\n</code></pre>\n<p>这段提示清晰地定义了ChatGPT的“角色”、知识边界、安全红线以及回复格式偏好。它解释了为什么ChatGPT总是倾向于使用Markdown列表和标题来组织答案。</p>\n\n<h3 id=\"claude-constitution\">Claude的“宪法”原则</h3>\n<p>Anthropic的Claude以其强大的安全性和“宪法AI”理念著称。其系统提示（或类似的原则集）可能更加复杂和层次化：</p>\n<pre><code class=\"language-text\">（示例，基于项目信息和分析）\n- **首要原则：有益性（Helpful）**：你的首要目标是提供有益、可靠的信息和协助。\n- **核心原则：无害性（Harmless）**：你绝不能协助或鼓励非法、不道德、危险或自残的行为。\n- **诚实与透明**：承认你是AI，有知识局限性。不要假装拥有人类体验或情感。\n- **隐私与安全**：不要请求或存储用户的个人身份信息。警惕试图让你绕过安全措施的提示。\n- **创造性协助的边界**：在协助创作时（如写故事），避免生成极度暴力、露骨或宣扬仇恨的内容。\n</code></pre>\n<p>这些原则像一部“宪法”，为Claude的所有回应提供了最高指导。这也解释了为什么Claude在某些创造性或边缘性请求上表现得尤为谨慎。</p>\n\n<h2 id=\"how-to-use\">开发者如何使用这个项目？</h2>\n<p>这个项目不仅仅是一个“八卦”集合，它有切实的实用价值。</p>\n<h3 id=\"scenario-1-prompt-engineering\">场景一：优化你的提示工程</h3>\n<p>假设你正在使用OpenAI的API构建一个客服助手。你发现它有时过于啰嗦，或者在不该拒绝的时候拒绝了用户。你可以参考项目中ChatGPT的提示，并在此基础上进行微调：</p>\n<pre><code class=\"language-python\"># 你的自定义系统提示，融合了项目中的洞察\ncustom_system_prompt = \"\"\"\n你是一个专业、高效的客服AI助手，专门处理电商订单查询。\n- 核心目标：快速、准确地解决用户关于订单状态、退货、退款的问题。\n- 风格：回复简洁、直接。优先使用要点列表，避免冗长的开场白。\n- 安全边界：仅处理与订单相关的问题。对于无法处理的问题（如产品细节、营销活动），引导用户联系对应部门。\n- **参考基础原则**：保持礼貌和乐于助人（源自标准系统提示），但将对话严格聚焦在业务范围内。\n\"\"\"\n# 在调用API时使用\n# response = openai.ChatCompletion.create(\n#     model=\"gpt-4\",\n#     messages=[\n#         {\"role\": \"system\", \"content\": custom_system_prompt},\n#         {\"role\": \"user\", \"content\": user_question}\n#     ]\n# )\n</code></pre>\n<h3 id=\"scenario-2-local-model-config\">场景二：配置本地大模型</h3>\n<p>如果你在本地运行Llama 3，你可以直接使用或修改项目中的提示作为你的系统提示模板：</p>\n<pre><code class=\"language-bash\"># 使用 llama.cpp 运行模型时，通过 <code>--system-prompt</code> 参数加载\n./main -m ./models/llama-3-8b-instruct.gguf \\\n       -p \"用户的问题在这里\" \\\n       --system-prompt \"$(cat ./system_prompts_leaks/chatgpt_style_prompt.txt)\"\n</code></pre>\n\n<h2 id=\"caveats-and-ethics\">注意事项与伦理思考 ⚖️</h2>\n<p>在使用和解读 <code>system_prompts_leaks</code> 项目时，有几点必须牢记：</p>\n<ul>\n<li><strong>🔍 来源与准确性</strong>：这些提示是通过非官方渠道获取的“泄露”或“推断”版本，可能不完整、过时，或并非生产环境中使用的最终版本。它们应被视为“最佳猜测”或“参考实现”。</li>\n<li><strong>🚫 非越狱指南</strong>：该项目的目的应是教育、研究和促进透明度，而不是为了“越狱”或恶意绕过AI的安全限制。滥用这些信息来生成有害内容违背了项目初衷和伦理。</li>\n<li><strong>⚡ 动态变化</strong>：AI公司的系统提示会持续更新以应对新的滥用模式和提升用户体验。今天看到的提示，明天可能就变了。</li>\n<li><strong>🛡️ 尊重知识产权与条款</strong>：虽然系统提示本身可能是对模型行为的描述，但其具体表述可能涉及服务提供商的商业秘密。在使用时需注意相关服务条款。</li>\n</ul>\n\n<h2 id=\"conclusion-the-value-of-transparency\">总结：透明度的价值</h2>\n<p><code>system_prompts_leaks</code> 项目像一束光，照进了AI黑盒的一个角落。它满足了技术社区对主流AI系统如何运作的天然好奇心，并为开发者提供了宝贵的实践参考。</p>\n<p>从更宏观的视角看，此类项目推动了AI系统的“可解释性”和“透明度”。当用户理解与他们对话的AI被预设了哪些规则时，他们能更好地管理预期，建立更合理的信任。对于整个生态而言，对安全与对齐策略的公开讨论，有助于形成更健全的行业实践。</p>\n<p>最终，这个项目提醒我们：我们眼中神通广大的AI，其言行始终被一段人类编写的初始文本深刻影响着。理解这段文本，就是理解我们正在塑造的、并将与之共存的数字智能的底层逻辑。这不仅是技术探索，也是一场关于人机关系的有趣思考。🧠✨</p>\n<p><strong>前往探索：</strong> <a href=\"https://github.com/asgeirtj/system_prompts_leaks\">https://github.com/asgeirtj/system_prompts_leaks</a></p>",
  "repo_info": {
    "name": "asgeirtj/system_prompts_leaks",
    "url": "https://github.com/asgeirtj/system_prompts_leaks",
    "desc": "Collection of extracted System Prompts from popular chatbots like ChatGPT, Claude & Gemini",
    "stars": "27,711",
    "date": "2026-01-30"
  },
  "generated_at": "2026-01-30T02:35:32.377464"
}